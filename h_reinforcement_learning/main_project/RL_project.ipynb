{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reinforcement Learning for Algorithmic Trading\n",
        "\n",
        "\n",
        "In this project, you will implement an algorithmic trading bot using reinforcement learning, specifically employing the actor-critic method with policy gradients. The primary goal is to develop a bot that can make profitable trading decisions by learning from historical financial data. The actor-critic architecture combines two neural networks: the actor, which decides the actions, and the critic, which evaluates those actions. We use simple multi-layer perceptrons (MLPs) for both networks, but in practice, recurrent networks (RNNs) are often preferred for their ability to handle time-series data more effectively.\n",
        "\n",
        "You will be working with the gym-trading-env package to simulate a trading environment. This environment provides various trading-related observations and actions, enabling the bot to learn and adapt its strategies. For more details, refer to the [documentation](https://gym-trading-env.readthedocs.io/en/latest/index.html) of the trading environment.\n",
        "\n",
        "While this project focuses on a simplified implementation, it serves as a foundational step toward solving more complex, practical problems. Even if the final trading bot does not perform optimally, the experience gained from working with reinforcement learning algorithms and trading environments is invaluable. It introduces you to crucial tools and concepts that are widely used in both academic research and industry applications.\n"
      ],
      "metadata": {
        "id": "5qz2G8J4l0Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-trading-env"
      ],
      "metadata": {
        "id": "0ChEE9NRehLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=1, hidden_dim=32):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        '''\n",
        "        TODO: Initialize layers for the actor network.\n",
        "        Use nn.Linear for linear layers and nn.ReLU for activation.\n",
        "        '''\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        '''\n",
        "        TODO: Convert the input state to a torch tensor if it is not already,\n",
        "        and pass it through the sequential model to get the prediction.\n",
        "        '''\n",
        "\n",
        "\n",
        "        return pred\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=1, hidden_dim=32):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        '''\n",
        "        TODO: Initialize layers for the critic network.\n",
        "        Use nn.Linear for linear layers and nn.ReLU for activation.\n",
        "        '''\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        '''\n",
        "        TODO: Convert the input state to a torch tensor if it is not already,\n",
        "        and pass it through the model to get the value prediction.\n",
        "        '''\n",
        "\n",
        "\n",
        "        return v_pred"
      ],
      "metadata": {
        "id": "kwbJuijQdVlg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PG_Agent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.98):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "        '''\n",
        "        TODO: Initialize the actor and critic networks with state_size and action_size.\n",
        "        '''\n",
        "\n",
        "\n",
        "        '''\n",
        "        TODO: Initialize the loss function and optimizers for the actor and critic networks using torch.optim.Adam.\n",
        "        '''\n",
        "\n",
        "\n",
        "    def sample_action(self, state):\n",
        "\n",
        "        '''\n",
        "        TODO:\n",
        "        Get action probabilities from the actor network, apply softmax,\n",
        "        and sample an action based on the probabilities.\n",
        "        '''\n",
        "\n",
        "        return action\n",
        "\n",
        "    def calc_reward_to_go(self, rewards):\n",
        "        '''\n",
        "        TODO: Calculate discounted future rewards.\n",
        "        Initialize a running sum and iterate through rewards in reverse to compute rewards-to-go.\n",
        "        '''\n",
        "\n",
        "\n",
        "        return rewards2go\n",
        "\n",
        "\n",
        "    def update(self, states, actions, rewards, max_batch=512):\n",
        "        # Process batch if needed\n",
        "        if len(states) > max_batch:\n",
        "            st_ind = np.random.choice(len(states) - max_batch, 1).item()\n",
        "            end_ind = st_ind + max_batch\n",
        "            actions = actions[st_ind: end_ind]\n",
        "            rewards = rewards[st_ind: end_ind]\n",
        "            states = states[st_ind: end_ind]\n",
        "\n",
        "        actions = torch.tensor(actions)\n",
        "        rewards2go = self.calc_reward_to_go(rewards)\n",
        "        rewards2go = torch.tensor(rewards2go)\n",
        "\n",
        "        # Update Critic network\n",
        "        '''\n",
        "        TODO: Zero the gradients, perform a forward pass to get values, compute the loss,\n",
        "        and perform backward pass and optimization step for the critic network.\n",
        "        '''\n",
        "\n",
        "\n",
        "        # Update Actor network\n",
        "        '''\n",
        "        TODO: Zero the gradients, perform a forward pass to get logits,\n",
        "        compute the advantages, log probabilities, actor loss, and perform backward pass and optimization step for the actor network.\n",
        "        '''\n"
      ],
      "metadata": {
        "id": "cR4gY8JmdeeJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym_trading_env.downloader import download\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "data_dir = 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download and prepare your data\n",
        "# Fetch historical BTC/USDT data from the Bitfinex exchange\n",
        "# Timeframe is set to 1 hour and data is saved in the 'data' directory\n",
        "# Data collection starts from January 1, 2021\n",
        "\n",
        "download(exchange_names=[\"bitfinex2\"], symbols=[\"BTC/USDT\"], timeframe=\"1h\", dir=\"data\",\n",
        "         since=datetime.datetime(year=2021, month=1, day=1))\n",
        "\n"
      ],
      "metadata": {
        "id": "l532ZqObdfIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_pickle(\"./data/bitfinex2-BTCUSDT-1h.pkl\")\n",
        "\n",
        "# Create features for the trading environment\n",
        "df[\"feature_pct_change\"] = df[\"close\"].pct_change()\n",
        "df[\"feature_high\"] = df[\"high\"] / df[\"close\"] - 1\n",
        "df[\"feature_low\"] = df[\"low\"] / df[\"close\"] - 1\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# Initialize the trading environment\n",
        "env = gym.make(\"TradingEnv\",\n",
        "        name= \"BTCUSD\",\n",
        "        df = df, # Your dataset with your custom features\n",
        "        positions = [ -1, 0, 1], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
        "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
        "        borrow_interest_rate = 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
        "        windows = 20) # Past N observations\n",
        "\n",
        "obs_dim = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
        "# Initialize the policy gradient agent\n",
        "agent = PG_Agent(obs_dim, env.action_space.n)\n",
        "\n",
        "n_episodes = 500\n",
        "total_reward = []\n",
        "\n",
        "for i in tqdm(range(n_episodes)):\n",
        "    state, info = env.reset()\n",
        "\n",
        "    episode_states = []\n",
        "    episode_rewards = []\n",
        "    episode_actions = []\n",
        "\n",
        "    done, truncated = False, False\n",
        "    max_steps = 10000 # Maximum number of steps per episode\n",
        "    step = 0\n",
        "    while not done and not truncated and step < max_steps:\n",
        "\n",
        "        '''\n",
        "        TODO: Flatten the state for the agent, sample an action from the policy,\n",
        "        interact with the environment using the sampled action,\n",
        "        and store the state, action, and reward.\n",
        "        '''\n",
        "\n",
        "\n",
        "        state = next_state\n",
        "        step += 1\n",
        "\n",
        "    # Update the agent after the episode\n",
        "    agent.update(np.array(episode_states), np.array(episode_actions), episode_rewards)\n",
        "    total_reward.append(np.sum(episode_rewards))\n",
        "\n",
        "# Plot the smoothed total rewards over episodes\n",
        "plt.plot(np.convolve(total_reward, np.ones(20) / 20)[10: -10])\n",
        "plt.title('Smoothed Rewards')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kvEMJ0hnoPDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}